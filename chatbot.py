# -*- coding: utf-8 -*-
"""Retrieval-Augmented Conversational AI Using FAISS and GPT-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ykJcTwc9oWmsUIHrnhMuTKaWaNgJl5mt
"""

pip install datasets transformers faiss-cpu sentence-transformers

import faiss
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from sentence_transformers import SentenceTransformer
from datasets import load_dataset

device = torch.device('cpu')

embedder = SentenceTransformer('all-MiniLM-L6-v2')
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
gpt_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)

print("Loading Wikipedia dataset...")
wiki_dataset = load_dataset('wikipedia', '20220301.en', split='train[:1%]')  # Using only 1% for demonstration

documents = [article['text'] for article in wiki_dataset]

def truncate_document(doc, max_length=1000):
    return doc[:max_length]

# Truncate all documents to a max length
documents = [truncate_document(doc) for doc in documents]

def create_faiss_index(docs):
    print("Creating embeddings...")

    embeddings = embedder.encode(docs, convert_to_tensor=True, device=device)
    embeddings = embeddings.numpy()

    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index, embeddings

index, doc_embeddings = create_faiss_index(documents)

def retrieve_documents(query, index, documents, k=2):
    query_embedding = embedder.encode(query, convert_to_tensor=True, device=device)
    query_embedding = query_embedding.numpy()

    distances, indices = index.search(query_embedding.reshape(1, -1), k)
    retrieved_docs = [documents[i] for i in indices[0]]
    return retrieved_docs

def generate_response(query, retrieved_docs, max_input_length=1024, max_new_tokens=150):
    context = " ".join(retrieved_docs)
    input_text = f"Context: {context}\nUser: {query}\nBot:"
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

    if input_ids.shape[1] > max_input_length:
        input_ids = input_ids[:, -max_input_length:]

    output = gpt_model.generate(
        input_ids,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )

    # Decoding the generated text
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

def rag_chatbot(query):
    retrieved_docs = retrieve_documents(query, index, documents)
    response = generate_response(query, retrieved_docs)
    return response

user_query = "Tell me about Fyodor Doestoevsky."
bot_response = rag_chatbot(user_query)
print("Bot:", bot_response)